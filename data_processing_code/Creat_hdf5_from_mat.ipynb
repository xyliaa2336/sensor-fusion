{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#DAVIS frames\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import imageio\n",
    "from scipy.io import loadmat\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.patches as patches\n",
    "import Image\n",
    "import glob\n",
    "#directory where the mat files are stored\n",
    "DAVIS_path='/media/yy/My Passport/lipread_data/correlated_spikegrams/'\n",
    "name_list= glob.glob('/media/yy/My Passport/lipread_data/correlated_spikegrams/*_normed.mat')\n",
    "#print out some info about a file\n",
    "print(name_list[0])\n",
    "print(name_list[0][-30:-21])\n",
    "vid = loadmat(name_list[0])\n",
    "print(vid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load .mat files and convert to npy files\n",
    "\n",
    "for filename in name_list:\n",
    "\n",
    "    vid = loadmat(filename)\n",
    "    if len(vid)<4:\n",
    "        continue\n",
    "        \n",
    "    frames=np.asarray(vid['DVS_normed'])\n",
    "        out_data=frames\n",
    "  \n",
    "    output_folder = DAVIS_path+'np_data/'\n",
    "\n",
    "    np.save(output_folder+filename[-30:-21]+'_dvs.npy', out_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cochlea\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import imageio\n",
    "from scipy.io import loadmat\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.patches as patches\n",
    "import Image\n",
    "import glob\n",
    "\n",
    "#directory where the mat files are stored\n",
    "coch_path='/media/yy/Seagate Expansion Drive1/hw_grid_mat/DVS_frames_40_on-off/'\n",
    "name_list= glob.glob('/media/yy/Seagate Expansion Drive1/DVS_frames_40_norm_part4/*_normed.mat')\n",
    "\n",
    "#print out some info about a file\n",
    "print(name_list[2000])\n",
    "print(name_list[2000][-31:-21])\n",
    "aud = loadmat(name_list[0])\n",
    "print(aud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load .mat files and convert to npy files\n",
    "for filename in name_list:\n",
    "    try:\n",
    "        aud = loadmat(filename)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    if len(aud)<3:\n",
    "        continue\n",
    "    if filename[-15]=='/':\n",
    "        continue\n",
    "        \n",
    "        \n",
    "    vect=np.asarray(aud['cochvector'])\n",
    "\n",
    "    out_data=vect\n",
    "    #out_data=frames.reshape((len(vid['allTsd_frames']),48,48,1))\n",
    "    '''plt.figure()\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.imshow(out_data[1],cmap='Greys_r')\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.imshow(out_data[7],cmap='Greys_r')\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.imshow(out_data[15],cmap='Greys_r')\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.imshow(out_data[20],cmap='Greys_r')'''\n",
    "    output_folder = coch_path+'np_data/'\n",
    "\n",
    "    np.save(output_folder+filename[-31:-21]+'_coch.npy', out_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create hdf5 file \n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "#directory that stores the npy files\n",
    "VID_DATA_DIR = '/media/yy/My Passport/lipread_data/correlated_spikegrams/np_data/'\n",
    "#directory that stores the key files, providing the name of files to be loaded\n",
    "# the key files are used to synchronization between different type of recordings\n",
    "KEY_DIR = '/media/yy/My Passport/lipread_data/correlated_spikegrams/np_data/'\n",
    "np.random.seed(26)\n",
    "subset_extract_ratio=0.1\n",
    "train_list, test_list = [], []\n",
    "list_of_vids = glob.glob(KEY_DIR+'*.npy')\n",
    "for vid in list_of_vids:\n",
    "    key = vid.split('/')[-1].split('_')[0]\n",
    "    if np.random.random() < subset_extract_ratio:\n",
    "        test_list.append(key)\n",
    "    else:\n",
    "        train_list.append(key)\n",
    "        \n",
    "print('{} keys for training, {} keys for test.'.format(\n",
    "        len(train_list), len(test_list)))\n",
    "\n",
    "from lipreading_utils import all_vocab, word_to_idx, idx_to_word, vocab_size\n",
    "print('Done.')\n",
    "print(test_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File('/media/yy/My Passport/lipread_data/current_used_datasets/DVS_corr.hdf5', 'w')\n",
    "#train_aud_grp = f.create_group(\"train_aud\")\n",
    "#test_aud_grp  = f.create_group(\"test_aud\")\n",
    "#train_coch_grp = f.create_group(\"train_coch\")\n",
    "#test_coch_grp  = f.create_group(\"test_coch\")\n",
    "train_vid_grp = f.create_group(\"train_dvs\")\n",
    "test_vid_grp  = f.create_group(\"test_dvs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "final_size = (48, 48)\n",
    "train_list_new=[]\n",
    "test_list_new=[]\n",
    "miss_cnt=0\n",
    "tic=0\n",
    "for key in test_list:\n",
    "    tic=tic+1\n",
    "    if(tic%1000==0):\n",
    "        print('.', end=\"\")\n",
    "    try:\n",
    "        vid_data = np.load(VID_DATA_DIR+key+'_dvs.npy')\n",
    "    except Exception as e:\n",
    "        miss_cnt+=1\n",
    "        continue\n",
    "\n",
    "    test_list_new.append(key)\n",
    "    test_vid_grp.create_dataset(key, data=vid_data)\n",
    "    \n",
    "tic=0\n",
    "for key in train_list:\n",
    "    tic=tic+1\n",
    "    if(tic%1000==0):\n",
    "        print('.', end=\"\") \n",
    "    try:\n",
    "        vid_data = np.load(VID_DATA_DIR+key+'_dvs.npy')\n",
    "    except Exception as e:\n",
    "        miss_cnt+=1\n",
    "        continue\n",
    "    train_list_new.append(key)\n",
    "    train_vid_grp.create_dataset(key, data=vid_data)\n",
    "    \n",
    "\n",
    "train_labels  = f.create_dataset(\"train_labels\", data=np.array(train_list_new))\n",
    "test_labels   = f.create_dataset(\"test_labels\", data=np.array(test_list_new))    \n",
    "f.close()\n",
    "print(miss_cnt)\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compute mean and std sample. Dont need this procedure for event data\n",
    "f = h5py.File('/media/yy/My Passport/lipread_data/grid_subsetII/Grid_video.hdf5', \"r\")\n",
    "# Audio\n",
    "#mean_aud   = np.zeros(64)\n",
    "#std_aud    = np.zeros(64)\n",
    "#sum_sq_aud = np.zeros(64)\n",
    "#tot_audio  = 0\n",
    "# Video\n",
    "mean_vid   = np.zeros((48,48))\n",
    "std_vid    = np.zeros((48,48))\n",
    "sum_sq_vid = np.zeros((48,48))\n",
    "tot_video  = 0\n",
    "\n",
    "# Get means\n",
    "for l_idx, label in enumerate(f['train_labels']):\n",
    "    # Audio\n",
    "    #aud_data   = np.array(f['train_coch'][label])\n",
    "    #print(aud_data.shape)\n",
    "    #print(label)\n",
    "    #mean_aud  += np.sum(aud_data, axis=1)\n",
    "    #tot_audio += aud_data.shape[1]\n",
    "    # Video\n",
    "    vid_data   = np.array(f['train_vid'][label])\n",
    "    mean_vid  += np.squeeze(np.sum(np.mean(vid_data, axis=3), axis=0))\n",
    "    #mean_vid  += np.squeeze(np.sum(vid_data, axis=0))\n",
    "    tot_video += vid_data.shape[0]\n",
    "    if (l_idx+1)%1000 == 0:\n",
    "        print('.'),        \n",
    "mean_vid /= tot_video\n",
    "#mean_aud /= tot_audio\n",
    "print('Done with means.')\n",
    "# Get std\n",
    "\n",
    "for l_idx, label in enumerate(f['train_labels']):\n",
    "    # Audio\n",
    "    #aud_data   = np.array(f['train_coch'][label])\n",
    "    #sum_sq_aud+= np.sum((np.transpose(aud_data)-mean_aud)**2, axis=0)\n",
    "    # Video\n",
    "    vid_data   = np.array(f['train_vid'][label])\n",
    "    sum_sq_vid+= np.sum((np.mean(vid_data, axis=3)-mean_vid)**2, axis=0)\n",
    "    if (l_idx+1)%1000 == 0:\n",
    "        print('.'), \n",
    "print('Done with stds.')\n",
    "#std_aud = np.sqrt(sum_sq_aud / tot_video)\n",
    "std_vid = np.sqrt(sum_sq_vid / tot_video)\n",
    "\n",
    "f.close()\n",
    "\n",
    "plt.figure(figsize=(16,7))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(mean_vid, interpolation='nearest', cmap='gray')\n",
    "plt.colorbar()\n",
    "\n",
    "#plt.subplot(1,2,2)\n",
    "#plt.plot(mean_aud)\n",
    "#plt.grid(which='both')\n",
    "\n",
    "plt.figure(figsize=(16,7))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(std_vid, interpolation='nearest', cmap='gray')\n",
    "plt.colorbar()\n",
    "\n",
    "#plt.subplot(1,2,2)\n",
    "#plt.plot(std_aud)\n",
    "#plt.grid(which='both')\n",
    "\n",
    "plt.figure(figsize=(16,7))\n",
    "plt.subplot(1,2,1)\n",
    "vid_img = vid_data[0]\n",
    "plt.imshow((vid_img-mean_vid)/std_vid, interpolation='nearest', cmap='gray')\n",
    "plt.colorbar()\n",
    "'''\n",
    "plt.subplot(1,2,2)\n",
    "curr_aud = aud_data[0]\n",
    "plt.plot((np.transpose(curr_aud)-mean_aud)/std_aud)\n",
    "plt.grid(which='both')'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
